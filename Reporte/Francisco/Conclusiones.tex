El objetivo del apartado de clasificación fue desarrollar un modelo con los conocimientos y técnicas adquiridos a lo largo del \textit{Samsung Innovation Campus}, que pudiera distinguir entre tres tipos de cáncer de pulmón, dada una imagen histopatológica del tumor. Para esto se propusieron 8 diferentes modelos, 3 contenidos en la libreria \textit{sklearn} y 5 modelos de redes neuronales, desarrollados con la ayuda de las librerías \textit{keras} y \textit{tensorflow}. Estos modelos fueron evaluados con diferentes métricas para medir y cuantificar su desempeño, algunas métricas fueron: el \textit{accuracy}, la matriz de confusión, las gráficas \textit{Training vs Validation} entre otras. \\

En la sección de resultados del apartado de clasificación, se muestra la \hyperref[tabla:resultados_modelos]{\textbf{Tabla comparativa}} de las principales métricas de estos modelos. Podemos observar que el modelo de \textit{Transfer Learning} con \textit{MobileNetV2} fue el modelo con mejores métricas, consiguiendo un 0.99 de \textit{precision}, \textit{recall} y \textit{F1-score}. Esto podría ser indicio de que este modelo fue el que obtuvo mejores resultados, pero al ser valores casi perfectos, es probable que exista un sobre ajuste a los datos de entrenamiento y por lo tanto el modelo no generalice bien con imágenes ajenas al \textit{dataset} de entrenamiento. Los modelos de \textit{XGBClassifier}, la CNN de \textit{Transfer Learning} con \textit{VGG16} y la CNN propuesta, al tener los 3 métricas superiores a 0.9, es más probable que tengan una mejor generalización de las características aprendidas del conjunto de entrenamiento. Los modelos con peor desempeño fueron la Regresión Logística y el modelo de \textit{Transfer Learning} con ResNetRS101, teniendo el primero métricas de 0.68 y 0.69, y el segundo métricas de 0.63. El modelo de \textit{Support Vector Classifier} tuvo un desempeño moderado acercándose a bueno con métricas de 0.88.

Al observar las diferentes matrices de confusión y los reportes de clasificación, podemos distinguir una mayor facilidad de todos los modelos para clasificar los tumores benignos. Todos tuvieron dificultades en la distinción de las clases adenocarcinoma y carcinoma escamocelular, aún cuando las imágenes se convirtieron a escala de grises justamente para evitar esto. Aunque, al observar diferentes imágenes del \textit{dataset} usado, sí existe una clara diferencia entre las imágenes de tumor benigno y las otras 2 clases a simple vista. 

Las diferencias de desempeño entre los modelos son coherentes con lo visto a lo largo del curso. El modelo de Regresión Logística es uno sencillo y poco potente para tareas de clasificación sencilla, por lo que su bajo desempeño en la clasificación de imágenes, no es una sorpresa. El modelo de \textit{Transfer Learning} con \textit{ResNetRS101} tuvo un desempeño pobre, esto es debido probablemente a las dimensiones de la red y a la cantidad de datos pues se trató de una cantidad moderada. Su posterior mejoría usando la técnica de \textit{Fine Tuning}, era de esperarse, pues una mayor cantidad de pesos permitió que se adaptará mejor a los datos de entrenamiento. El \textit{Support Vector Classifier} tuvo un desempeño aceptable, como era de esperarse para uno de los modelos más robustos de \textit{Machine Learning}. El probable sobre ajuste del modelo de \textit{Transfer Learning} con \textit{MobileNetV2}, sí fue un poco sorprendente, pues se trata de un modelo pequeño. El sobre ajuste podría deberse a que más de la mitad de los parámetros de la red completa, eran parámetros entrenables, y el resto estaban pre-entrenados, pero esto es solo una hipótesis. Por último, el resto de los modelos tuvó un excelente desempeño sin llegar a un sobre ajuste: del modelo \textit{XGBClassifier} era de esperarse pues se trata de un modelo robusto de \textit{Machine Learning}, el modelo de \textit{Transfer Learning} con \textit{VGG16} suele tener resultados pobres, pero sorprendentemente, obtuvo buenos resultados y la CNN propuesta era un modelo de complejidad media y de tamaño mediano que se adaptó desde cero a los datos de entrenamiento. Este último modelo es un claro ejemplo del poder computacional de los modelos de \textit{Deep Learning} a la hora de realizar tareas de clasificación, incluso con imágenes. \\

Algo que nos pareció pertinente comentar, es la homogeneidad de las imágenes en el \textit{dataset}. Si uno observa ejemplos de imágenes de las 3 clases, se puede observar que absolutamente todas parecen tener exactamente el mismo acercamiento en el microscopio. Lo que podría impedir una mayor generalización de los modelos al intentar predecir imágenes tomadas con diferentes objetivos (\textit{zoom}) en el microscopio, sin importar el tipo de tumor de la fotografía. \\

\newpage

Al haber trabajado con un \textit{dataset} que no es exclusivo del cáncer de pulmón, pues cuenta también con imágenes de cáncer de colon, fue complicado hallar estudios similares al nuestro en cuanto a modelos de clasificación compete. Pero se logró hallar estudios que trabajaron, al menos una parte de estos, únicamente con cáncer de pulmón, he aquí un cuadro comparativo entre esos modelos y los propuestos en este proyecto:

\begin{table}[H]
    \centering
    \begin{tabular}{lcccc}
        \toprule
        Modelo              & Accuracy & Precision & Recall & F1-score \\
        \midrule
        SVC                 & 0.88 & 0.88 & 0.88 & 0.88 \\
        Regresión Logística & 0.68 & 0.68 & 0.69 & 0.68 \\
        XGBClassifier       & \textbf{0.90} & \textbf{0.90} & \textbf{0.90} & \textbf{0.90} \\
        TL MobileNetV2      & 0.99 & 0.99 & 0.99 & 0.99 \\
        TL VGG16            & \textbf{0.97} & \textbf{0.97} & \textbf{0.97} & \textbf{0.97} \\
        TL ResNetRS101      & 0.63 & 0.63 & 0.63 & 0.63 \\
        FT ResNetRS101      & 0.72 & 0.72 & 0.73 & 0.72 \\
        CNN propuesta       & 0.91 & 0.91 & 0.91 & 0.91 \\
        CNN ({\cite{CNN1}}) & 0.972 & 0.9733 & 0.9733 & 0.9733 \\
        CNN ({\cite{CNN2}}) & 0.9789 & - & - & - \\
        XGBClassifier ({\cite{modelo_XGBClassifier}}) & 0.9953 & 0.9933 & 0.9933 & 0.9933 \\
        \bottomrule
    \end{tabular}
    \caption{Tabla comparativa de métricas de los modelos propuestos y de otros estudios}
\end{table}

Podemos observar que las métricas del modelo de \textit{Transfer Learning} con \textit{MobileNetV2}, superó los modelos de CNN propuestos en otros estudios. Pero como se señaló anteriormente, se teme un sobre ajuste de este modelo, por lo que consideraremos el segundo mejor modelo, el modelo de \textit{Transfer Learning} con \textit{VGG16}. Este modelo tiene aproximadamente las mismas métricas que los modelos propuestos en otros estudios, por lo que nuestro modelo está dentro de lo esperado en la literatura. Por otro lado, para el modelo de \textit{XGBClassifier}, el modelo propuesto en otro estudio tuvo métricas casi perfectas y el propuesto en este proyecto tuvo métricas peores en comparación, aunque muy buenas. Teniendo en cuenta que la diferencia entre ambos modelos es de menos del 10\%, podemos decir que nos encontramos dentro del desempeño esperado.\\

Si hablamos de la aplicación de estos modelos en la práctica profesional, existen limitaciones. Se habló previamente de la homogeneidad de las imágenes del \textit{dataset}, parece que todas las fotografías fueron tomadas con el mismo objetivo del microscopio (\textit{zoom}). Lo que implica que los modelos solo saben clasificar imágenes con este grado de acercamiento, hipótesis que se comprobó al intentar predecir imágenes ajenas al \textit{dataset} con un grado de acercamiento diferente. Los modelos tuvieron una buena precisión con imágenes de objetivo similar, pero una muy baja con cualquier otro objetivo. Por lo que no se recomendaría utilizarlos en la práctica profesional, pues existiría una probabilidad importante de una negligencia médica. \\

Podemos decir con entusiasmo que existe oportunidad de mejora en el desempeño o generalización de estos modelos. Concerniente a los modelos de \textit{Machine Learning}, se pueden explorar otros modelos como \textit{KNN} o \textit{RandomForest} y realizar \textit{GridSearchCV} más extensos, con una cantidad de parámetros mayor, en consecuencia de combinaciones. Por el lado de los modelos de \textit{Deep Learning}, podemos en primer lugar realizar una validación cruzada a los modelos con posible sobre ajuste. Con estos modelos se pueden realizar diversas propuestas de \textit{CNN} tanto propias como con técnicas de \textit{Transfer Learning} y \textit{Fine Tuning}, y dado que existe una buena cantidad de modelos base, y podemos ``congelar'' y ``descongelar'' pesos de manera arbitraria, las posibilidades son abundantes.

Otra forma de mejorar la generalización de los modelos es mejorar la cantidad, y en este caso la diversidad de los datos. Ya se habló repetidamente de la homogeneidad existente en el \textit{dataset}. Agregar una mayor variabilidad de imágenes histopatológicas de estos tipos de cáncer, en particular, con diferentes objetivos (\textit{zoom}), enriquecería mucho los datos, provocando una probable mejoría en el desempeño de los modelos. 

Por último leyendo la literatura, se pueden usar técnicas de selección de características para mejorar el desempeño de los modelos. También se realizó una ``normalización'' de las imágenes dividiendo el valor de los píxeles entre 255, pero en la librería de \textit{tensorflow} existen otros métodos y funciones que se podrían usar para realizar una normalización más extensa, lo que podría generar un mayor desempeños de los modelos. \\

\newpage

En conclusión, en este proyecto pudimos aplicar los conocimientos adquiridos a lo largo del \textit{Samsung Innovation Campus} y también demuestra el potencial de los modelos de \textit{Machine Learning} y \textit{Deep Learning} en la clasificación de tipos de cánceres de pulmón, mostrando que la inteligencia artificial puede ser una herramienta poderosa en el diagnóstico médico. Sin embargo, aún quedan desafíos por resolver antes de su aplicación en entornos clínicos reales.\\